# 基础知识

> 涉及一些基础知识的总结与整理

## 1、机器学习

机器学习算法是一种能够从数据中学习的算法。Mitchell(1997)定义机器学习：“对于某类任务`T`和性能度量`P`，一个计算机程序被认为可以从经验`E`中学习是指通过经验`E`改进后，它在任务`T`上由性能度量`P`衡量的性能有所提升。” 

机器学习的主要挑战是我们的算法必须能够在先前未观测的新输入上表现良好，而不只是在训练集上表现良好。在先前未观测到的输入上表现良好的能力被称为 泛化（generalization）。

以下是决定机器学习算法效果是否好的因素：

1. 降低训练误差。
2. 缩小训练误差和测试误差的差距。

对应机器学习的两个主要挑战： 欠拟合（underfitting）和 过拟合（overfitting）。

## 2、预训练

我们在训练模型的时候，如果训练数据比较少，则会导致训练的模型参数不够好；这时候，如果有一批其它的大量的训练数据，我们可用这批大量的数据训练一个网络，学会网络参数，然后用相同的网络结构，参数初始化时加载训练好的参数，在高层的参数随机初始化，用少量的数据集进行训练。加载的训练好的参数可以保持不变，即“frozen“，也可以随着训练的进行改变，即”fine_tuning“，最常见的例子用在图像处理中。

优点：加快训练速度；解决数据集太小导致的无法训练问题；找到好的初始化参数。

## 3、自回归语言模型

在ELMO／BERT出来之前，大家通常讲的语言模型其实是根据上文内容预测下一个可能跟随的单词，就是常说的自左向右的语言模型任务，或者反过来也行，就是根据下文预测前面的单词，这种类型的LM被称为自回归语言模型。GPT 就是典型的自回归语言模型。

## 4、卷积网络示意图

![conv](../images/cnn/conv.gif)

## 5、KMP算法：判断一个字符串是否在另一个字符串里出现。

1. 第一位匹配，匹配不上后移一位，匹配上之后，继续判断第二位字符。
2. 多于一位字符匹配上之后，后边发现不匹配，原来的办法是继续后移一位，KMP算法后移的位数是：已匹配的字符数 - 对应的部分匹配值。

对应的部分匹配值计算方式：

 "前缀"指除了最后一个字符以外，一个字符串的全部头部组合；"后缀"指除了第一个字符以外，一个字符串的全部尾部组合。

"部分匹配值"就是"前缀"和"后缀"的最长的共有元素的长度。

> 详细内容可以看大神阮一峰的博客：[字符串匹配的KMP算法](http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html)，介绍的非常通俗易懂。

## 6、激活函数

![激活函数](../images/activation/activation_function.png)

> 图来自[pytorch-nlp-zh](https://nlp-pt.apachecn.org/docs/3.html)


## 7、CNN 做 nlp

CNN的卷积核是能保留特征之间的相对位置的，滑动窗口从左到右滑动，捕获到的特征也是如此顺序排列，所以在结构上已经记录了相对位置信息了。但是Max Pooling的操作逻辑是：从一个卷积核获得的特征向量里只选中并保留最强的那一个特征，所以如果卷积层后面立即接上Pooling层的话，到了Pooling层，位置信息就被扔掉了，这在NLP里其实是有信息损失的。所以在NLP领域里，目前CNN的一个发展趋势是抛弃Pooling层，靠全卷积层来叠加网络深度。

## 8、Bertrand Russell’s chicken

罗素鸡（Bertrand Russell’s chicken）只对现象进行统计和归纳，不对原因进行推理。

而主流的基于统计的机器学习特别是深度学习，也是通过大量的案例，靠对文本的特征进行归类，来实现对识别语义的效果。这个做法，就是罗素鸡。


# 参考内容



